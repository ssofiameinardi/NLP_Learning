{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 4 --> SHARING MODELS AND TOKENIZERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 5--> THE HF DATASETS LIBRARY\n",
    "\n",
    "- Load datasets from anywhere, be it the Hugging Face Hub, your laptop, or a remote server at your company.\n",
    "- Wrangle your data using a mix of the Dataset.map() and Dataset.filter() functions.\n",
    "- Quickly switch between data formats like Pandas and NumPy using Dataset.set_format().\n",
    "- Create your very own dataset and push it to the Hugging Face Hub.\n",
    "- Embed your documents using a Transformer model and build a semantic search engine using FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 6 --> THE HF TOKENIZERS LIBRARY\n",
    "\n",
    "- Training a new tokenizer from an old one\n",
    "- Fast tokenizers\n",
    "- Normalization and pre-tokenization\n",
    "- BPE, WordPiece and Unigram tokenization\n",
    "- Building a tokenizer block by block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 7 --> MAIN NLP TASKS\n",
    "\n",
    "- Token classification\n",
    "- Fine-tuning and masked language model\n",
    "- Translation\n",
    "- Summarization\n",
    "- Training a causal language model from scratch\n",
    "- Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 9"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
