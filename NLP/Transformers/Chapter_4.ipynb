{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huggingface_hub library Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like getting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositoriesâ€™ content and to integrate the Hub in your projects and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import (\n",
    "    # User management\n",
    "    login,\n",
    "    logout,\n",
    "    whoami,\n",
    "\n",
    "    # Repository creation and management\n",
    "    create_repo,\n",
    "    delete_repo,\n",
    "    update_repo_visibility,\n",
    "\n",
    "    # And some methods to retrieve/change information about the content\n",
    "    list_models,\n",
    "    list_datasets,\n",
    "    list_metrics,\n",
    "    list_repo_files,\n",
    "    upload_file,\n",
    "    delete_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Repository class manages a local repository in a git-like manner. It abstracts most of the pain points one may have with git to provide all features that we require.\n",
    "\n",
    "Using this class requires having git and git-lfs installed, so make sure you have git-lfs installed and set up before you begin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(\"<path_to_dummy_folder>\", clone_from=\"<namespace>/dummy-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repo.git_pull()\n",
    "repo.git_add()\n",
    "repo.git_commit()\n",
    "repo.git_push()\n",
    "repo.git_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model card is the central definition of the model, ensuring reusability by fellow community members and reproducibility of results, and providing a platform on which other members may build their artifacts. We create it through the README file\n",
    "The model card usually starts with a very brief, high-level overview of what the model is for, followed by additional details in the following sections:\n",
    "\n",
    "Model description\n",
    "Intended uses & limitations\n",
    "How to use\n",
    "Limitations and bias\n",
    "Training data\n",
    "Training procedure\n",
    "Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model description --> includes architecture, version, if it was introduced in a paper, if an original implementation is available, the author and general information about the model. Any copyright should be attributed here. General info about training procedures, parameters, important disclaimers.\n",
    "\n",
    "- Intended uses and limitations --> Description of the use cases the model is intended for, including languages, fields, domains where it can be applied. It also document areas that are known to be out of scope for the model or where it is likely to perform suboptimally.\n",
    "\n",
    "- How to use --> examples of how to use the model. This can showcase usage of the pipeline() function, usage of the model and tokenizer classes \n",
    "\n",
    "- Training data --> Should indicate which datasets the model was trained on. Brief description of dataset\n",
    "\n",
    "- Training procedure --> Should describe all the relevant aspects of training that are useful from a reproducibility perspective. Includes any preprocessing and postprocessing that were done on the data, as well as details such as the number of epochs the model was trained for, batch size, learning rate.\n",
    "\n",
    "- Variable and metrics --> Description of the metrics used for evaluation, and the different factors you are mesuring. It makes it easy to compare the model with others.\n",
    "\n",
    "-Evaluation results --> Provide an indication of how well the model performs on the evaluation dataset. Provide the decision threshold used in evaluation in case of needing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading local data sets:\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "#By default, loading local files creates a DatasetDict object with a train split. We can see this by inspecting the squad_it_dataset object:\n",
    "\n",
    "squad_it_dataset\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['title', 'paragraphs'],\n",
    "        num_rows: 442\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To include both the train and test splits in a single DatasetDict object so we can apply Dataset.map() functions across both splits at once, we can provide a dictionary to the data_files argument that maps each split name to a file associated with that split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset\n",
    "\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['title', 'paragraphs'],\n",
    "        num_rows: 442\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['title', 'paragraphs'],\n",
    "        num_rows: 48\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loading a remote dataset, Instead of providing a path to local files, we point the data_files argument of load_dataset() to one\n",
    "# or more URLs where the remote files are stored\n",
    "\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
