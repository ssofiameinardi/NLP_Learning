{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the manipulation if datasets and datasetDict, we'll be using the Drug Review Dataset, wich contains patient reviews on various frugs, along with the condition being treated in a 10-star rating of the patient's satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download and extract the data:\n",
    "\n",
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A small random sample of the dataset\n",
    "\n",
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response of the request is:\n",
    "\n",
    "{'Unnamed: 0': [87571, 178045, 80482],\n",
    " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
    " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
    " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
    "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
    "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
    " 'rating': [9.0, 3.0, 10.0],\n",
    " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
    " 'usefulCount': [36, 13, 128]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test the patient ID hypothesis for the Unnamed:0 column, we can use the \n",
    "# Dataset.unique() function to verify that the number of IDs matches the number \n",
    "# of rows in each split\n",
    "\n",
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed:0\"))\n",
    "\n",
    "#Since it confirms our hypothesis, we rename that column to something more interpretable\n",
    "\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed:0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "drug_dataset\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
    "        num_rows: 161297\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
    "        num_rows: 53766\n",
    "    })\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To normalize all the condition labels, first we need to drop the entries in the codition column that are \"none\".\n",
    "\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "#Now we can normalize the column using dataset.map()\n",
    "\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "#Check that lowecasing worked \n",
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple function that counts the number of words in each review\n",
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "\n",
    "#It returns a dictionary whose keys does not correspond to one of the column names in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use the dataset.filter() function to remove reviews that contain feweer than 30 words\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have to unescape html character codes in our reviews\n",
    "\n",
    "import html\n",
    "\n",
    "text = \"I&$#039;m feeling better now\"\n",
    "print(html.unescape(text))\n",
    "\n",
    "#The response should be \"\"I'm feeling better now\"\n",
    "\n",
    "\n",
    "#We'll use Dataset,map() to unescape all the HTML characters in our corpus\n",
    "\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is another way to unescape all HTML characters ut using batched = True\n",
    "\n",
    "new_drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched = True\n",
    ")\n",
    "\n",
    "#This command executes way faster than the previous one because list comprehensions are usually\n",
    "# faster than executing the same code in a for loop, and we also gain some preformance by accessing\n",
    "# lots of elements at the same time instead of one by one\n",
    "\n",
    "# Using Dataset.map() with batched=True will be essential to unlock the speed of the \"fast\" tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tokenize all the drug reviews with a fast tokenizer, we could use a function like this:\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable multiprocessing, use the num_proc argument and specify the number of processes to use\n",
    "# in your call to dataset.map()\n",
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer \n",
    "# to return all the chunks of the texts instead of just the first one. This can be done with return_overflowing_tokens=True:\n",
    "\n",
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "[len(inp) for inp in result[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the test in one example is [128, 49]. \n",
    "When we run it in the whole dataset, we get en error because the length of the two columns are not the same: the drug_dataset column has a certain number of examples but the tokenized_dataset we are building will have more. That doesn't work for a dataset, so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset.\n",
    "We can do the former with the remove_columns argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = drug_dataset.map(\n",
    "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Now this works without error. \n",
    "#  We can check that our new dataset has many more elements than the original dataset by comparing the lengths\n",
    "\n",
    "print(len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"]))\n",
    "\n",
    "# [206772, 138514]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to deal with the mismatched length problem is by making the old columns the same size as the new ones\n",
    "# To do this we will need the overflow_to_sample_mapping field the tokenizer returns when we set return_overflowing_tokens=True\n",
    "\n",
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "#We can see it works with Dataset.map() without us needing to remove the old columns:\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "tokenized_dataset\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
    "        num_rows: 206772\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
    "        num_rows: 68876\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a dataset into a dataframe, as Pandas, we can do it as:\n",
    "\n",
    "--> drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "And when we access elements of the dataset we get a pandas.DataFrame instead of a dictionary\n",
    "We can create a pandas.DataFrame for the whole training set by selecting all the elements of drug_dataset[\"train\"]:\n",
    "\n",
    "--> train_df = drug_dataset[\"train\"][:]\n",
    "\n",
    "From here we can use all the Pandas functionality that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we're done with our Pandas analysis, we can always create a new Dataset object by using the Dataset.from_pandas() function as:\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset\n",
    "\n",
    "Dataset({\n",
    "    features: ['condition', 'frequency'],\n",
    "    num_rows: 819\n",
    "})\n",
    "\n",
    "#To reset the output format of drug_dataset from \"pandas\" to \"arrow\":\n",
    "\n",
    "drug_dataset.reset_format()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a validation set: to split our training set into train and\n",
    "# validation splits (we set the seed argument for reproducibility):\n",
    "\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],\n",
    "        num_rows: 110811\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],\n",
    "        num_rows: 27703\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],\n",
    "        num_rows: 46108\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store somewhere the dataset, we use:\n",
    "    - In Arrow format: Dataset.save_to_disk()\n",
    "    - in CSV format: Dataset.to_csv()\n",
    "    - in JSON format: Dataset.to_json()\n",
    "\n",
    "ex: drug_dataset_clean.save_to_disk(\"drug-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the dataset is saved, we can load it by using the load_from_disk() function as follows:\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "drug_dataset_reloaded\n",
    "\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
    "#        num_rows: 110811\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
    "#        num_rows: 27703\n",
    "#    })\n",
    "#    test: Dataset({\n",
    "#        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
    "#        num_rows: 46108\n",
    "#    })\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For csv and json format, we have to store each split as a separate file. One way to do this is by iterating over the keys\n",
    "# and values in the DatasetDict object\n",
    "\n",
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")\n",
    "\n",
    "# Other way: \n",
    "data_files = {\n",
    "    \"train\": \"drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'v just installed The Pile (pip install zstandard)\n",
    "Now I can load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\n",
    "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "pubmed_dataset\n",
    "\n",
    "#Dataset({\n",
    "#    features: ['meta', 'text'],\n",
    "#    num_rows: 15518009\n",
    "#})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to measure memory usage in Pyhton is with the psutil library\n",
    "\n",
    "pip install psutil\n",
    "\n",
    "import psutil\n",
    "\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the management of memory when working with big datasets, hugging face's datasets provides a streaming feature that allow us to download and access elements on the fly, without needing to download the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To enable dataset streaming just pass the streaming=Truen arg to the load_dataset() function\n",
    "pubmed_dataset_streamed = load_dataset(\n",
    "    \"json\", data_files=data_files, split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "#What it returns is a iterableDataset. We can access the first element of our streamed dataset as:\n",
    "next(iter(pubmed_dataset_streamed))\n",
    "\n",
    "#The elements from a streamed dataset can be processed on the fly using IterableDataset.map(), which is \n",
    "# useful during training if you need to tokenize the inputs\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
    "next(iter(tokenized_dataset))\n",
    "\n",
    "#OUTPUT\n",
    "#{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}\n",
    "\n",
    "#To speed up tokenization with streaming you can pass batched=True, which would process the examples batch by batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements from a streamed dataset can be selected using the IterableDataset.take() and IterableDataset.skip() functions, which act in a similar way to Dataset.select(). Example:\n",
    "\n",
    "dataset_head = pubmed_dataset_streamed.take(5)\n",
    "list(dataset_head)\n",
    "\n",
    "OUTPUT:\n",
    "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
    "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},\n",
    " {'meta': {'pmid': 11409575, 'language': 'eng'},\n",
    "  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},\n",
    " {'meta': {'pmid': 11409576, 'language': 'eng'},\n",
    "  'text': \"Hypoxaemia in children with severe pneumonia in Papua New Guinea ...\"},\n",
    " {'meta': {'pmid': 11409577, 'language': 'eng'},\n",
    "  'text': 'Oxygen concentrators and cylinders ...'},\n",
    " {'meta': {'pmid': 11409578, 'language': 'eng'},\n",
    "  'text': 'Oxygen supply in rural africa: a personal experience ...'}]\n",
    "\n",
    "Similarly, we can use the IterableDataset.skip() function to create training and validation splits from a shuffled dataset as follows:\n",
    "\n",
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = shuffled_dataset.take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to combine larga datasets, we can use the interleave_datasets() function that converts a list of iterableDataset objects into a single iterableDataset, where the elements of the new dataset are obtained by alternating among the source examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\n",
    "list(islice(combined_dataset, 2))\n",
    "\n",
    "#OUTPUT\n",
    "#[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
    "#  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},\n",
    "# {'meta': {'case_ID': '110921.json',\n",
    "#   'case_jurisdiction': 'scotus.tar.gz',\n",
    "#   'date_created': '2010-04-28T17:12:49Z'},\n",
    "#  'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\n",
    "# \\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy\n",
    "# Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to stream the dataset in its 825 GB enitrety, you can grab all the prepared files:\n",
    "\n",
    "base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n",
    "    \"validation\": base_url + \"val.jsonl.zst\",\n",
    "    \"test\": base_url + \"test.jsonl.zst\",\n",
    "}\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "next(iter(pile_dataset[\"train\"]))\n",
    "\n",
    "#OUTPUT\n",
    "#{'meta': {'pile_set_name': 'Pile-CC'},\n",
    "# 'text': 'It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web...'}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
